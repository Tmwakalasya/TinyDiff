# TinyDiff

TinyDiff is a lightweight automatic differentiation library inspired by micrograd. It provides a simple and educational implementation of reverse-mode autodiff, useful for understanding the fundamentals of neural networks and backpropagation.

## Features

- Scalar-valued autograd engine
- Basic arithmetic operations (addition, subtraction, multiplication, division, exponentiation)
- readable implementation for educational purposes

## Installation

Clone this repository:
git clone https://github.com/Tmwakalasya/TinyDiff.git
cd TinyDiff




## Project Structure

tinydiff.py: Core implementation of the Value class and operations
examples/: Directory containing usage examples (to be added)
tests/: Unit tests for the library (to be added)

## Contributing
Contributions to TinyDiff are welcome! Please feel free to submit a Pull Request.

## To-Do
Implement backward pass for automatic differentiation
Add more operations (e.g., logarithm, exponential)
Create visualization function for computational graphs
Write unit tests
Add examples demonstrating usage in simple neural networks

License
This project is licensed under the MIT License - see the LICENSE file for details.
Acknowledgments

Inspired by Andrej Karpathy's micrograd


